{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline model - simple MLP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "SEED = 42\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hard = pd.read_excel('all_data_long.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29128"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hard['Fictive ID'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = copy.deepcopy(df_hard)\n",
    "df.columns = df.columns.str.replace(\"'\", \"\")\n",
    "features_to_start = ['year', 'מין', 'סוג פיקוח', ' פסיכומטרי רב תחומי', 'פסיכומטרי כמותי', 'פסיכומטרי מילולי',\n",
    "               'שפת פסיכומטרי', 'בגרות', 'ציון מכינה', 'ציון אנגלית', 'ניקוד רל\"ק', 'שנת לידה', 'פקולטה', 'חוג', 'מסלול', 'משקולל קיים', 'נשר חיצונית']\n",
    "df = df[features_to_start]\n",
    "\n",
    "feature_names_to_replace = {\"מין\": \"gender\", 'סוג פיקוח': 'supervision_type',\n",
    "                        ' פסיכומטרי רב תחומי': 'psico_rav', 'פסיכומטרי כמותי':'psico_camul',\n",
    "                        'פסיכומטרי מילולי':'psico_verb', 'שפת פסיכומטרי': 'psico_leng',\n",
    "                        'בגרות':'bag', 'ציון מכינה':'preparatory_score', 'ציון אנגלית':'eng_score', 'ניקוד רל\"ק':'ralak_score',\n",
    "                        'שנת לידה':'birth_year', 'פקולטה':'faculty',\n",
    "                        'חוג':'hug', 'מסלול':'study_track', 'משקולל קיים':'weighted_score', 'נשר חיצונית':'dropouts'}\n",
    "\n",
    "df = df.rename(columns=feature_names_to_replace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split to train and test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('dropouts', axis=1)\n",
    "Y = df['dropouts'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=SEED)\n",
    "\n",
    "# Reset indices for compatibility\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_train = pd.Series(y_train).reset_index(drop=True)\n",
    "y_test = pd.Series(y_test).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build base model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropuotsDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'features': torch.tensor(self.features.iloc[idx].values, dtype=torch.float32),\n",
    "            'label': torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModelNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(BaseModelNN, self).__init__()\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(input_dim, 12)\n",
    "        self.fc2 = nn.Linear(12, 6)\n",
    "        self.output = nn.Linear(6, 1)\n",
    "\n",
    "    def forward(self, features):\n",
    "        x = torch.relu(self.fc1(features))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.output(x))  # For binary classification\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = X_train.select_dtypes(include=['number']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader\n",
    "batch_size = 32\n",
    "train_dataset = DropuotsDataset(X_train[numeric_features], y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = DropuotsDataset(X_test[numeric_features], y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialize model and optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaseModelNN(input_dim=X_train[numeric_features].shape[1])\n",
    "criterion = nn.BCELoss()  # For binary classification\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, epochs, device):\n",
    "    model.to(device)  # Move model to the specified device (CPU or GPU)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            # Move data to the same device as the model\n",
    "            features = batch['features'].to(device)\n",
    "            labels = batch['label'].to(device).view(-1, 1)  # Ensure labels are shaped as (batch_size, 1)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate the loss\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Calculate average loss for the epoch\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 96.4461\n",
      "Epoch [2/10], Loss: 96.4461\n",
      "Epoch [3/10], Loss: 96.4448\n",
      "Epoch [4/10], Loss: 96.4461\n",
      "Epoch [5/10], Loss: 96.4435\n",
      "Epoch [6/10], Loss: 96.4435\n",
      "Epoch [7/10], Loss: 96.4461\n",
      "Epoch [8/10], Loss: 96.4448\n",
      "Epoch [9/10], Loss: 96.4461\n",
      "Epoch [10/10], Loss: 96.4448\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, criterion, optimizer, epochs=10, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, criterion):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    total_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            # Move data to the same device as the model\n",
    "            features = batch['features'].to(device)\n",
    "            labels = batch['label'].to(device).view(-1, 1)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Get predictions and probabilities\n",
    "            probabilities = outputs.squeeze().cpu().numpy()  # Convert to numpy array\n",
    "            predictions = (probabilities >= 0.5).astype(int)  # Threshold for binary classification\n",
    "            labels = labels.cpu().numpy()  # Convert labels to numpy\n",
    "\n",
    "            # Store results\n",
    "            all_labels.extend(labels)\n",
    "            all_predictions.extend(predictions)\n",
    "            all_probs.extend(probabilities)\n",
    "\n",
    "    # Compute average loss\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    precision = precision_score(all_labels, all_predictions)\n",
    "    recall = recall_score(all_labels, all_predictions)\n",
    "    f1 = f1_score(all_labels, all_predictions)\n",
    "    roc_auc = roc_auc_score(all_labels, all_probs)\n",
    "\n",
    "    print(f\"Evaluation Results:\")\n",
    "    print(f\"Loss: {avg_loss:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "    return {'loss': avg_loss,\n",
    "            'accuracy_score': accuracy,\n",
    "            'precision_score': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'roc_auc': roc_auc,\n",
    "            'lables': all_labels,\n",
    "            'predictions': all_predictions,\n",
    "            'proabilities': all_probs}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results:\n",
      "Loss: 96.2411\n",
      "Accuracy: 0.0377\n",
      "Precision: 0.0377\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.0726\n",
      "ROC AUC: 0.5000\n"
     ]
    }
   ],
   "source": [
    "test_eval_results = evaluate_model(model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{np.int64(1)}"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(test_eval_results['predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this model practically predicts all students will drop. this is obviousley not good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhancements: \n",
    "change the model architecture and preprocess data to achive better results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add layers and neurons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPModelNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(BaseModelNN, self).__init__()\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(input_dim, 32)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.fc3 = nn.Linear(16, 8)\n",
    "        self.fc4 = nn.Linear(8, 4)\n",
    "        self.fc5 = nn.Linear(4, 2)\n",
    "        self.output = nn.Linear(2, 1)\n",
    "\n",
    "    def forward(self, features):\n",
    "        x = torch.relu(self.fc1(features))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        x = torch.relu(self.fc5(x))\n",
    "        x = torch.sigmoid(self.output(x))  # For binary classification\n",
    "        return x\n",
    "\n",
    "\n",
    "model = BaseModelNN(input_dim=X_train[numeric_features].shape[1])\n",
    "criterion = nn.BCELoss()  # For binary classification\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 49.3531\n",
      "Epoch [2/10], Loss: 3.5577\n",
      "Epoch [3/10], Loss: 3.5539\n",
      "Epoch [4/10], Loss: 3.5539\n",
      "Epoch [5/10], Loss: 3.5552\n",
      "Epoch [6/10], Loss: 3.5577\n",
      "Epoch [7/10], Loss: 3.5565\n",
      "Epoch [8/10], Loss: 3.4994\n",
      "Epoch [9/10], Loss: 3.3926\n",
      "Epoch [10/10], Loss: 3.5552\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, criterion, optimizer, epochs=10, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results:\n",
      "Loss: 3.7589\n",
      "Accuracy: 0.9623\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "ROC AUC: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nadavkat\\OneDrive - Intel Corporation\\Documents\\Learning\\University\\University-Dropouts-Prediction\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "test_eval_results = evaluate_model(model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like adding layers dosent solve the problem, and the model is still not preforming well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess data:\n",
    "The data preprocess is based on knowledge about this specific usecase and data, as revised by an SME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    df = data.copy()\n",
    "    \n",
    "    # encode categorical binary variables:\n",
    "    df['gender'] = np.where(df['gender'] == 'ז', 0, 1)\n",
    "    df['study_track'] = np.where(df['study_track'] == 'חד חוגי', 0, 1)\n",
    "    \n",
    "    # remove samples with 0 bagrut score:\n",
    "    df = df[df['bag'] != 0]# remove 1252 samples\n",
    "    \n",
    "    df['supervision_type'] = np.where(df['supervision_type'].isna(), 'missing', df['supervision_type'])\n",
    "    \n",
    "    # df['ralak'] = np.where(df['ralak_score'] >= 30, 1, 0)\n",
    "    # df.pop('ralak_score')\n",
    "    \n",
    "    # learned in preparatory at huji:\n",
    "    df['preparatory'] = np.where(df['preparatory_score'] > 0, 1, 0)\n",
    "    df.pop('preparatory_score')\n",
    "    \n",
    "    # create age feature:\n",
    "    df['age'] = df['year'] - df['birth_year']\n",
    "    df.pop('birth_year')\n",
    "    # df.pop('חוג')\n",
    "    # df.pop('פקולטה')\n",
    "    \n",
    "    # create dummy variables for hug:\n",
    "    df = pd.get_dummies(df, columns=['hug'], dtype=float)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep = preprocess_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_prep.drop('dropouts', axis=1)\n",
    "Y = df_prep['dropouts'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=SEED)\n",
    "\n",
    "# Reset indices for compatibility\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_train = pd.Series(y_train).reset_index(drop=True)\n",
    "y_test = pd.Series(y_test).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader\n",
    "numeric_features = X_train.select_dtypes(include=['number']).columns\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train[numeric_features] = scaler.fit_transform(X_train[numeric_features])\n",
    "X_test[numeric_features] = scaler.transform(X_test[numeric_features])\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "train_dataset = DropuotsDataset(X_train[numeric_features], y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = DropuotsDataset(X_test[numeric_features], y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPModelNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(MLPModelNN, self).__init__()\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 16)\n",
    "        self.fc4 = nn.Linear(16, 8)\n",
    "        self.fc5 = nn.Linear(8, 4)\n",
    "        self.output = nn.Linear(4, 1)\n",
    "\n",
    "    def forward(self, features):\n",
    "        x = torch.relu(self.fc1(features))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        x = torch.relu(self.fc5(x))\n",
    "        x = torch.sigmoid(self.output(x))  # For binary classification\n",
    "        return x\n",
    "\n",
    "\n",
    "model = MLPModelNN(input_dim=X_train[numeric_features].shape[1])\n",
    "criterion = nn.BCELoss()  # For binary classification\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.1614\n",
      "Epoch [2/10], Loss: 0.1520\n",
      "Epoch [3/10], Loss: 0.1512\n",
      "Epoch [4/10], Loss: 0.1509\n",
      "Epoch [5/10], Loss: 0.1490\n",
      "Epoch [6/10], Loss: 0.1492\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, criterion, optimizer, epochs=10, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results:\n",
      "Loss: 0.1451\n",
      "Accuracy: 0.9672\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "ROC AUC: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nadavkat\\OneDrive - Intel Corporation\\Documents\\Learning\\University\\University-Dropouts-Prediction\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "test_eval_results = evaluate_model(model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{np.int64(0)}"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(test_eval_results['predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
